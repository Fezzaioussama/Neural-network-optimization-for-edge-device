{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMW1F76ieY2Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "#NAS PARAMETERS\n",
        "CONTROLLER_SAMPLING_EPOCHS = 10\n",
        "SAMPLES_PER_CONTROLLER_EPOCH = 20\n",
        "CONTROLLER_TRAINING_EPOCHS = 5\n",
        "ARCHITECTURE_TRAINING_EPOCHS = 5\n",
        "CONTROLLER_LOSS_ALPHA = 0.9\n",
        "\n",
        "#CONTROLLER(LSTM) PARAMETERS\n",
        "CONTROLLER_LSTM_DIM = 100\n",
        "CONTROLLER_OPTIMIZER = 'Adam'\n",
        "CONTROLLER_LEARNING_RATE = 0.01\n",
        "CONTROLLER_DECAY = 0.1\n",
        "CONTROLLER_MOMENTUM = 0.0\n",
        "CONTROLLER_USE_PREDICTOR = False\n",
        "\n",
        "#CNN ARCH PARAMETERS\n",
        "MAX_ARCHITECTURE_LENGTH = 8\n",
        "MLP_DECAY = 0.0\n",
        "MLP_MOMENTUM = 0.0\n",
        "MLP_LOSS_FUNCTION = 'categorical_crossentropy'\n",
        "MLP_ONE_SHOT = True\n",
        "\n",
        "#As using MNIST dataset for checking of CNN training and testing\n",
        "TARGET_CLASSES = 10\n",
        "TOP_N = 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.optimizers import *\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Dropout, Conv2D, SeparableConv2D, DepthwiseConv2D, Conv2DTranspose\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
        "#import CNNCONSTANTS\n",
        "#from CNNCONSTANTS import *\n",
        "class CNNSearchSpace(object):\n",
        "\n",
        "    def __init__(self, target_classes):\n",
        "\n",
        "        self.target_classes = target_classes\n",
        "        self.vocab = self.vocab_dict()\n",
        "\n",
        "\n",
        "    def vocab_dict(self):\n",
        "\n",
        "        #---------------------------Hyperparameter pool selection----------------------------------#\n",
        "\n",
        "        #For fully connected\n",
        "        nodes = [8, 16, 32, 64, 128, 256, 512]\n",
        "        act_funcs = ['sigmoid', 'tanh', 'relu', 'elu', 'selu', 'swish']\n",
        "\n",
        "        #For Convolutional Layers\n",
        "        conv_layers=['conv2d','separableconv2d','depthwiseconv2d','conv2dtranspose']\n",
        "        conv_filter_size=[3,5,7,9]\n",
        "        conv_filters=[16,32,64,96,128,160,192,224,256]\n",
        "        conv_padding= ['same','valid']\n",
        "        conv_stride=[2,3]\n",
        "        conv_weight_initializers=['HeNormal','HeUniform','RandomNormal','RandomUniform']\n",
        "        conv_bias_initializers=['HeNormal','HeUniform','RandomNormal','RandomUniform']\n",
        "        conv_regularizers=['l1','l2','l1_l2']\n",
        "\n",
        "        #For Pooling Layers\n",
        "        pool_layers=['maxpool2d','avgpool2d','globalmaxpool2d','globalavgpool2d']\n",
        "        pool_size=[2,3,4,5]\n",
        "        pool_stride=[1,2,3,4,5]\n",
        "        pool_padding=['same','valid']\n",
        "\n",
        "        #RegularizationLayers\n",
        "        #reg_layers=['dropout','spatialDropout','alphaDropout']\n",
        "        reg_layers=['dropout']\n",
        "        dropout_rate=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "\n",
        "        #Learning rate (Not included in the Search Space, this we will use in the training part on our own, that's why semi-automatic)\n",
        "        self.lr=[0.1,0.2,0.3,0.4,0.5,0.6]\n",
        "        self.batch_size=[2,4,8,16,32]\n",
        "        self.learning_optimimzer=['adam','rms','sgd']\n",
        "\n",
        "        # initialize lists for keys and values of the vocabulary\n",
        "        layer_params = []\n",
        "        layer_id = []\n",
        "\n",
        "        #---------------Starting creation of Voacb from which Controller will create a sequence-------------------------#\n",
        "\n",
        "        ind=1\n",
        "\n",
        "        for a in conv_layers:\n",
        "          for b in conv_filters:\n",
        "            for c in conv_filter_size:\n",
        "              for d in conv_stride:\n",
        "                for e in conv_padding:\n",
        "                  for f in conv_weight_initializers:\n",
        "                    for g in conv_bias_initializers:\n",
        "                      for h in conv_regularizers:\n",
        "                        if a is 'depthwiseconv2d':\n",
        "                            layer_params.append((a,c,d,e,f,g,h))\n",
        "                            layer_id.append(ind)\n",
        "                            ind=ind+1\n",
        "                        else:\n",
        "                            layer_params.append((a,b,c,d,e,f,g,h))\n",
        "                            layer_id.append(ind)\n",
        "                            ind=ind+1\n",
        "\n",
        "        self.conv_id=ind-1\n",
        "        for a in pool_layers:\n",
        "          for b in pool_size:\n",
        "            for c in pool_stride:\n",
        "              for d in pool_padding:\n",
        "                  if a==\"globalavgpool2d\" or a==\"globalmaxpool2d\":\n",
        "                      layer_params.append((a))\n",
        "                      layer_id.append(ind)\n",
        "                      ind+=1\n",
        "                  else:\n",
        "                      layer_params.append((a,b,c,d))\n",
        "                      layer_id.append(ind)\n",
        "                      ind=ind+1\n",
        "\n",
        "        self.pool_id=ind-1\n",
        "        for i in range(len(nodes)):\n",
        "            for j in range(len(act_funcs)):\n",
        "                layer_params.append((nodes[i], act_funcs[j]))\n",
        "                layer_id.append(ind)\n",
        "                ind=ind+1\n",
        "\n",
        "        self.fully_id=ind-1\n",
        "        for a in reg_layers:\n",
        "          for b in dropout_rate:\n",
        "            layer_params.append((a,b))\n",
        "            layer_id.append(ind)\n",
        "            ind=ind+1\n",
        "\n",
        "        self.reg_layer_id=ind-1\n",
        "        # zip the id and configurations into a dictionary\n",
        "        vocab = dict(zip(layer_id, layer_params))\n",
        "\n",
        "        # add Flatten and BatchNormalization in the volcabulary\n",
        "        vocab[len(vocab)+1] = (('Flatten'))\n",
        "        vocab[len(vocab) + 1] = (('BatchNormalization'))\n",
        "\n",
        "        # add the final softmax/sigmoid layer in the vocabulary\n",
        "        if self.target_classes == 2:\n",
        "            vocab[len(vocab) + 1] = (self.target_classes - 1, 'sigmoid')\n",
        "        else:\n",
        "            vocab[len(vocab) + 1] = (self.target_classes, 'softmax')\n",
        "        return vocab\n",
        "\n",
        "#--------------------------------------------Search Space Created--------------------------------------------#\n",
        "\n",
        "\t# function to encode a sequence of configuration tuples\n",
        "    def encode_sequence(self, sequence):\n",
        "        keys = list(self.vocab.keys())\n",
        "        values = list(self.vocab.values())\n",
        "        encoded_sequence = []\n",
        "        for value in sequence:\n",
        "            encoded_sequence.append(keys[values.index(value)])\n",
        "        return encoded_sequence\n",
        "\n",
        "\n",
        "\t# function to decode a sequence back to configuration tuples\n",
        "    def decode_sequence(self, sequence):\n",
        "        keys = list(self.vocab.keys())\n",
        "        values = list(self.vocab.values())\n",
        "        decoded_sequence = []\n",
        "        for key in sequence:\n",
        "            decoded_sequence.append(values[keys.index(key)])\n",
        "        return decoded_sequence\n",
        "\n",
        "class CNNGenerator(CNNSearchSpace):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.target_classes = TARGET_CLASSES\n",
        "        self.mlp_decay= MLP_DECAY\n",
        "        self.mlp_momentum= MLP_MOMENTUM\n",
        "        self.mlp_loss_func = MLP_LOSS_FUNCTION\n",
        "        self.mlp_one_shot = MLP_ONE_SHOT\n",
        "        self.metrics = ['accuracy']\n",
        "\n",
        "        super().__init__(TARGET_CLASSES)\n",
        "\n",
        "        if self.mlp_one_shot:\n",
        "\n",
        "            # path to shared weights file\n",
        "            self.weights_file = 'LOGS2/shared_weights.pkl'\n",
        "\n",
        "            # open an empty dataframe with columns for bigrams IDs and weights\n",
        "            self.shared_weights = pd.DataFrame({'bigram_id': [], 'weights': []})\n",
        "\n",
        "            # pickle the dataframe\n",
        "            if not os.path.exists(self.weights_file):\n",
        "                print(\"Initializing shared weights dictionary...\")\n",
        "                self.shared_weights.to_pickle(self.weights_file)\n",
        "\n",
        "    # function to create a keras model given a sequence and input data shape\n",
        "    def create_model(self, sequence, cnn_input_shape):\n",
        "\n",
        "            # decode sequence to get nodes and activations of each layer\n",
        "            layer_configs = self.decode_sequence(sequence)\n",
        "            try:\n",
        "                # create a sequential model\n",
        "                model = Sequential()\n",
        "\n",
        "                for i, layer_conf in enumerate(layer_configs):\n",
        "                    if i==0:\n",
        "                        if layer_conf[0] is 'conv2d':\n",
        "                            model.add(Conv2D(filters=layer_conf[1],kernel_size=layer_conf[2],strides=layer_conf[3],padding=layer_conf[4],kernel_initializer=layer_conf[5], bias_initializer=layer_conf[6], kernel_regularizer=layer_conf[7], input_shape=cnn_input_shape))\n",
        "                            continue\n",
        "                        elif layer_conf[0] is 'separableconv2d':\n",
        "                            model.add(SeparableConv2D(filters=layer_conf[1],kernel_size=layer_conf[2],strides=layer_conf[3],padding=layer_conf[4],depthwise_initializer=layer_conf[5], bias_initializer=layer_conf[6], depthwise_regularizer=layer_conf[7], input_shape=cnn_input_shape))\n",
        "                            continue\n",
        "                        elif layer_conf[0] is 'depthwiseconv2d':\n",
        "                            model.add(DepthwiseConv2D(kernel_size=layer_conf[1],strides=layer_conf[2],padding=layer_conf[3],depthwise_initializer=layer_conf[4], bias_initializer=layer_conf[5], depthwise_regularizer=layer_conf[6], input_shape=cnn_input_shape))\n",
        "                            continue\n",
        "                        else:\n",
        "                            model.add(Conv2DTranspose(filters=layer_conf[1],kernel_size=layer_conf[2],strides=layer_conf[3],padding=layer_conf[4],kernel_initializer=layer_conf[5], bias_initializer=layer_conf[6], kernel_regularizer=layer_conf[7], input_shape=cnn_input_shape))\n",
        "                            continue\n",
        "                    elif layer_conf[0] is 'conv2d':\n",
        "                        model.add(Conv2D(filters=layer_conf[1],kernel_size=layer_conf[2],strides=layer_conf[3],padding=layer_conf[4],kernel_initializer=layer_conf[5], bias_initializer=layer_conf[6], kernel_regularizer=layer_conf[7]))\n",
        "                    elif layer_conf[0] is 'separableconv2d':\n",
        "                        model.add(SeparableConv2D(filters=layer_conf[1],kernel_size=layer_conf[2],strides=layer_conf[3],padding=layer_conf[4],depthwise_initializer=layer_conf[5], bias_initializer=layer_conf[6], depthwise_regularizer=layer_conf[7]))\n",
        "                    elif layer_conf[0] is 'depthwiseconv2d':\n",
        "                        model.add(DepthwiseConv2D(kernel_size=layer_conf[1],strides=layer_conf[2],padding=layer_conf[3],depthwise_initializer=layer_conf[4], bias_initializer=layer_conf[5], depthwise_regularizer=layer_conf[6]))\n",
        "                    elif layer_conf[0] is 'conv2dtranspose':\n",
        "                        model.add(Conv2DTranspose(filters=layer_conf[1],kernel_size=layer_conf[2],strides=layer_conf[3],padding=layer_conf[4],kernel_initializer=layer_conf[5], bias_initializer=layer_conf[6], kernel_regularizer=layer_conf[7]))\n",
        "                    elif layer_conf[0] is 'maxpool2d':\n",
        "                        model.add(MaxPooling2D(pool_size=(layer_conf[1],layer_conf[1]),strides=(layer_conf[2],layer_conf[2]),padding=layer_conf[3]))\n",
        "                    elif layer_conf[0] is 'avgpool2d':\n",
        "                        model.add(AveragePooling2D(pool_size=(layer_conf[1],layer_conf[1]),strides=(layer_conf[2],layer_conf[2]),padding=layer_conf[3]))\n",
        "                    elif layer_conf[0] is 'globalmaxpool2d':\n",
        "                        model.add(GlobalMaxPooling2D())\n",
        "                    elif layer_conf[0] is 'globalavgpool2d':\n",
        "                        model.add(GlobalAveragePooling2D())\n",
        "                    # add subsequent layers (Dense or Dropout)\n",
        "                    elif layer_conf is 'dropout':\n",
        "                        model.add(Dropout(layer_conf[1], name='dropout'))\n",
        "                    elif layer_conf is 'Flatten':\n",
        "                        model.add(Flatten())\n",
        "                    elif layer_conf is 'BatchNormalization':\n",
        "                        model.add(BatchNormalization())\n",
        "                    else:\n",
        "                        model.add(Dense(units=layer_conf[0], activation=layer_conf[1]))\n",
        "\n",
        "            #print(model.summary())\n",
        "            # return the keras model\n",
        "                return model\n",
        "            except ValueError:\n",
        "                #print(\"-----------------------Received model that gets negative values for input image after processing------------------\")\n",
        "                return None\n",
        "\n",
        "    # function to compile the model with the appropriate optimizer and loss function\n",
        "    def compile_model(self, model):\n",
        "        models=[]\n",
        "        # Learning rate and Optimizer are changed and model is complied multiple times\n",
        "            # compile model\n",
        "\n",
        "        for i in self.lr:\n",
        "            for j in self.learning_optimimzer:\n",
        "                if j is 'sgd':\n",
        "                    #optim = tensorflow.keras.optimizers.SGD(lr=i, decay=self.mlp_decay, momentum=self.mlp_momentum)\n",
        "                    optim = tensorflow.keras.optimizers.SGD(learning_rate=i, momentum=self.mlp_momentum)\n",
        "                elif j is 'adam':\n",
        "                    #optim=tensorflow.keras.optimizers.Adam( lr=i, decay=self.mlp_decay)\n",
        "                    optim = tensorflow.keras.optimizers.Adam(learning_rate=i)\n",
        "                else:\n",
        "                    #optim=tensorflow.keras.optimizers.RMSprop(lr=i, decay=self.mlp_decay)\n",
        "                    optim = tensorflow.keras.optimizers.RMSprop(learning_rate=i)\n",
        "                model.compile(loss=self.mlp_loss_func, optimizer=optim, metrics=self.metrics)\n",
        "                models.append(model)\n",
        "            #------------------Always check whether the loss function and metrics is in accordance with the target classes and the dataset-------------#\n",
        "            # return a list of compiled keras model\n",
        "\n",
        "        #optim=tensorflow.keras.optimizers.Adam(lr=0.2,decay=self.mlp_decay)\n",
        "        #model.compile(loss=self.mlp_loss_func,optimizer=optim,metrics=self.metrics)\n",
        "        return models\n",
        "\n",
        "\n",
        "    def set_model_weights(self, model):\n",
        "        #print(model)\n",
        "        # get nodes and activations for each layer\n",
        "        layer_configs=[]\n",
        "        for layer in model.layers:\n",
        "            #print(layer.name)\n",
        "            #print(layer.get_config())\n",
        "            # add flatten since it affects the size of the weights\n",
        "            #index=layer.get_config()['name'].rfind(\"_\")\n",
        "            if 'flatten' in layer.name:\n",
        "                layer_configs.append((layer.input_shape,'Flatten'))\n",
        "            # don't add dropout since it doesn't affect weight sizes or activations\n",
        "            elif not (('dropout' in layer.name) or ('max_pooling2d' in layer.name) or ('average_pooling2d' in layer.name) or ('global_max_pooling2d' in layer.name) or ('global_average_pooling2d' in layer.name)):\n",
        "                #For Conv Layers\n",
        "                #if layer.name is 'conv2d'\n",
        "                #print(layer.name)\n",
        "                if 'separable_conv2d' in layer.name:\n",
        "                    layer_configs.append((layer.input_shape,'separable_conv2d',layer.get_config()['filters'],layer.get_config()['kernel_size']))\n",
        "                #elif layer.name is 'separableconv2d':\n",
        "                    #index=layer.get_config()['name'].rfind(\"_\")\n",
        "                    #if index == 9:\n",
        "                        #layer_configs.append(layer.get_config()['name'],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "                    #else :\n",
        "                     #   layer_configs.append(layer.get_config()['name'][:layer.get_config()['name'].rfind(\"_\")],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "                elif 'depthwise_conv2d' in layer.name:\n",
        "                    layer_configs.append((layer.input_shape,'depthwise_conv2d',layer.get_config()['kernel_size']))\n",
        "\n",
        "                #elif layer.name is 'depthwiseconv2d':\n",
        "                    #index=layer.get_config()['name'].rfind(\"_\")\n",
        "                    #if index == 9:\n",
        "                     #   layer_configs.append(layer.get_configs()['name'],layer.get_configs()['kernel_size'])\n",
        "                    #else:\n",
        "                       # layer_configs.append(layer.get_configs()['name'][:layer.get_configs()['name'].rfind('_')],layer.get_configs()['kernel_size'])\n",
        "                elif 'conv2d_transpose' in layer.name:\n",
        "                    layer_configs.append((layer.input_shape,'conv2d_transpose',layer.get_config()['filters'],layer.get_config()['kernel_size']))\n",
        "\n",
        "                #elif layer.name is 'conv2dtranspose':\n",
        "                    #index=layer.get_config()['name'].rfind(\"_\")\n",
        "                    #if index == 6:\n",
        "                     #   layer_configs.append(layer.get_config()['name'],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "                    #else :\n",
        "                     #   layer_configs.append(layer.get_config()['name'][:layer.get_config()['name'].rfind(\"_\")],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "\n",
        "                elif 'conv2d' in layer.name:\n",
        "                    layer_configs.append((layer.input_shape,'conv2d',layer.get_config()['filters'],layer.get_config()['kernel_size']))\n",
        "                    #index=layer.get_config()['name'].rfind(\"_\")\n",
        "                    #if index == -1:\n",
        "                        #layer_configs.append(layer.get_config()['name'],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "                    #else :\n",
        "                        #layer_configs.append(layer.get_config()['name'][:layer.get_config()['name'].rfind(\"_\")],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "\n",
        "                #For BatchNormalization Layer\n",
        "                elif layer.name is 'batch_normalization':\n",
        "                    layer_configs.append((layer.input_shape,'batch_normalization',layer.get_config()['filters'],layer.get_config()['kernel_size']))\n",
        "                    #layer_configs.append(layer.get_config()['name'][:layer.get_config()['name'].index(\"_\")],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "\n",
        "                #For Dense Layers\n",
        "                else :\n",
        "                    layer_configs.append((layer.input_shape,layer.get_config()['units'], layer.get_config()['activation']))\n",
        "\n",
        "        # get bigrams of relevant layers for weights transfer\n",
        "        config_ids = []\n",
        "        #Starting from 1 as we are using i-1 in the saving part\n",
        "        for i in range(1, len(layer_configs)):\n",
        "            config_ids.append((layer_configs[i - 1], layer_configs[i]))\n",
        "\n",
        "        # for all layers\n",
        "        j = 0\n",
        "        #print('---------------------------Setting Weights-------------------')\n",
        "        for i, layer in enumerate(model.layers):\n",
        "            if j >= len(config_ids):\n",
        "                break\n",
        "            if not (('dropout' in layer.name) or ('max_pooling2d' in layer.name) or ('average_pooling2d' in layer.name) or ('global_max_pooling2d' in layer.name) or ('global_average_pooling2d' in layer.name)):\n",
        "                warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "                # get all bigram values we already have weights for\n",
        "                bigram_ids = self.shared_weights['bigram_id'].values\n",
        "                #print(\"Layer : {0}------ Bigram :{1}\".format(i,bigram_ids))\n",
        "                # check if a bigram already exists in the dataframe\n",
        "                search_index = []\n",
        "                for x in range(len(bigram_ids)):\n",
        "                    #print(\"C0: \",config_ids[j][0][1:])\n",
        "                    #print(\"B0: \",bigram_ids[x][0][1:])\n",
        "                    #print(\"C1: \",config_ids[j][1])\n",
        "                    #print(\"B1: \",bigram_ids[x][1])\n",
        "                    #print(\"Config value of first:\",config_ids[j][0][0:])\n",
        "                    #print(\"Bigram value of first:\",bigram_ids[x][0][0:])\n",
        "                    if ((config_ids[j][0][1:] == bigram_ids[x][0][1:]) and (config_ids[j][1]==bigram_ids[x][1])):\n",
        "                        search_index.append(x)\n",
        "\n",
        "                # set layer weights if there is a bigram match in the dataframe\n",
        "                if len(search_index) > 0:\n",
        "                    #print(\"Transferring weights for layer:\", config_ids[j])\n",
        "                    layer.set_weights(self.shared_weights['weights'].values[search_index[0]])\n",
        "                j += 1\n",
        "\n",
        "    def update_weights(self, model):\n",
        "\n",
        "        # get nodes and activations for each layer\n",
        "        layer_configs = []\n",
        "        for layer in model.layers:\n",
        "            if 'flatten' in layer.name:\n",
        "                layer_configs.append((layer.input_shape,'Flatten'))\n",
        "            elif not (('dropout' in layer.name) or ('max_pooling2d' in layer.name) or ('average_pooling2d' in layer.name) or ('global_max_pooling2d' in layer.name) or ('global_average_pooling2d' in layer.name)):\n",
        "                if 'separable_conv2d' in layer.name:\n",
        "                    layer_configs.append((layer.input_shape,'separable_conv2d',layer.get_config()['filters'],layer.get_config()['kernel_size']))\n",
        "                elif 'depthwise_conv2d' in layer.name:\n",
        "                    layer_configs.append((layer.input_shape,'depthwise_conv2d',layer.get_config()['kernel_size']))\n",
        "                elif 'conv2d_transpose' in layer.name:\n",
        "                    layer_configs.append((layer.input_shape,'conv2d_transpose',layer.get_config()['filters'],layer.get_config()['kernel_size']))\n",
        "                elif 'conv2d' in layer.name:\n",
        "                    layer_configs.append((layer.input_shape,'conv2d',layer.get_config()['filters'],layer.get_config()['kernel_size']))\n",
        "                elif layer.name is 'batch_normalization':\n",
        "                    layer_configs.append((layer.input_shape,'batch_normalization',layer.get_config()['filters'],layer.get_config()['kernel_size']))\n",
        "                else :\n",
        "                    layer_configs.append((layer.input_shape,layer.get_config()['units'], layer.get_config()['activation']))\n",
        "\n",
        "            '''\n",
        "            # add flatten since it affects the size of the weights\n",
        "            if 'Flatten' in layer.name:\n",
        "                layer_configs.append(('Flatten'))\n",
        "            # don't add dropout since it doesn't affect weight sizes or activations\n",
        "            elif ('dropout' or 'maxpool2d' or 'avgpool2d' or 'globalmaxpool2d' or 'globalavgpool2d') not in layer.name:\n",
        "\n",
        "                #For Conv Layers\n",
        "                if layer.name is 'conv2d':\n",
        "                    index=layer.get_config()['name'].rfind(\"_\")\n",
        "                    if index == -1:\n",
        "                        layer_configs.append(layer.get_config()['name'],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "                    else :\n",
        "                        layer_configs.append(layer.get_config()['name'][:layer.get_config()['name'].rfind(\"_\")],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "                elif layer.name is 'separableconv2d':\n",
        "                    index=layer.get_config()['name'].rfind(\"_\")\n",
        "                    if index == 9:\n",
        "                        layer_configs.append(layer.get_config()['name'],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "                    else :\n",
        "                        layer_configs.append(layer.get_config()['name'][:layer.get_config()['name'].rfind(\"_\")],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "                elif layer.name is 'depthwiseconv2d':\n",
        "                    index=layer.get_config()['name'].rfind(\"_\")\n",
        "                    if index == 9:\n",
        "                        layer_configs.append(layer.get_configs()['name'],layer.get_configs()['kernel_size'])\n",
        "                    else:\n",
        "                        layer_configs.append(layer.get_configs()['name'][:layer.get_configs()['name'].rfind('_')],layer.get_configs()['kernel_size'])\n",
        "                elif layer.name is 'conv2dtranspose':\n",
        "                    index=layer.get_config()['name'].rfind(\"_\")\n",
        "                    if index == 6:\n",
        "                        layer_configs.append(layer.get_config()['name'],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "                    else :\n",
        "                        layer_configs.append(layer.get_config()['name'][:layer.get_config()['name'].rfind(\"_\")],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "\n",
        "                #For BatchNormalization Layer\n",
        "                elif layer.name is 'BatchNormalization':\n",
        "                    layer_configs.append(layer.get_config()['name'][:layer.get_config()['name'].index(\"_\")],layer.get_config()['filters'],layer.get_config()['kernel_size'])\n",
        "\n",
        "                #For Dense Layers\n",
        "                else :\n",
        "                    layer_configs.append((layer.get_config()['units'], layer.get_config()['activation']))\n",
        "        '''\n",
        "\n",
        "        # get bigrams of relevant layers for weights transfer\n",
        "        config_ids = []\n",
        "        for i in range(1, len(layer_configs)):\n",
        "            config_ids.append((layer_configs[i - 1], layer_configs[i]))\n",
        "\n",
        "        # for all layers\n",
        "        j = 0\n",
        "        #print('-------------------Updating weights--------------------------')\n",
        "        for i, layer in enumerate(model.layers):\n",
        "            if j >= len(config_ids):\n",
        "                break\n",
        "            if not (('dropout' in layer.name) or ('max_pooling2d' in layer.name) or ('average_pooling2d' in layer.name) or ('global_max_pooling2d' in layer.name) or ('global_average_pooling2d' in layer.name)):\n",
        "                warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "                #get all bigram values we already have weights for\n",
        "                bigram_ids = self.shared_weights['bigram_id'].values\n",
        "                #print(\"Layer : {0}------ Bigram :{1}\".format(i,bigram_ids))\n",
        "                # check if a bigram already exists in the dataframe\n",
        "                search_index = []\n",
        "                for x in range(len(bigram_ids)):\n",
        "                    if ((config_ids[j][0][1:] == bigram_ids[x][0][1:]) and (config_ids[j][1]==bigram_ids[x][1])):\n",
        "                        search_index.append(x)\n",
        "\n",
        "                # add weights to df in a new row if weights aren't already available\n",
        "                if len(search_index) == 0:\n",
        "                    self.shared_weights = self.shared_weights.append({'bigram_id': config_ids[j],\n",
        "                                                                      'weights': layer.get_weights()},\n",
        "                                                                     ignore_index=True)\n",
        "                # else update weights\n",
        "                else:\n",
        "                    self.shared_weights.at[search_index[0], 'weights'] = layer.get_weights()\n",
        "                j += 1\n",
        "        self.shared_weights.to_pickle(self.weights_file)\n",
        "\n",
        "\n",
        "\n",
        "    def train_model(self, models, x_data, y_data, nb_epochs, validation_split=0.1, callbacks=None):\n",
        "        history_of_models=None\n",
        "        val_acc=0\n",
        "        for model in models:\n",
        "            if self.mlp_one_shot:\n",
        "                self.set_model_weights(model)\n",
        "                for batch_size_value in self.batch_size:\n",
        "                    history = model.fit(x_data,\n",
        "                                y_data,\n",
        "                                epochs=nb_epochs,\n",
        "                                validation_split=validation_split,\n",
        "                                callbacks=callbacks,\n",
        "                                verbose=0)\n",
        "                    if history.history['val_accuracy'][0] > val_acc or val_acc==0:\n",
        "                        val_acc=history.history['val_accuracy'][0]\n",
        "                        history_of_models=history\n",
        "                        self.update_weights(model)\n",
        "            else:\n",
        "                for batch_size_value in self.batch_size:\n",
        "                    history = model.fit(x_data,\n",
        "                                y_data,\n",
        "                                epochs=nb_epochs,\n",
        "                                batch_sizze=batch_size_value,\n",
        "                                validation_split=validation_split,\n",
        "                                callbacks=callbacks,\n",
        "                                verbose=0)\n",
        "                    if history.history['val_accuracy'][0] > val_acc:\n",
        "                        val_acc=history.history['val_accuracy'][0]\n",
        "                        history_of_models=history\n",
        "        return history_of_models"
      ],
      "metadata": {
        "id": "37Pm4YgPej6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Mar 21 10:42:31 2022\n",
        "\n",
        "@author: AnshumaanChauhan\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten, Input\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.activations import relu\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "REPLAY_MEMORY_SIZE = 50000 # Can also write as 50_000 for readability\n",
        "MODEL_NAME= \"First_Try\"\n",
        "MIN_REPLAY_MEMORY_SIZE=1000\n",
        "MINIBATCH_SIZE= 64\n",
        "DISCOUNT= 0.99\n",
        "UPDATE_TARGET_EVERY=5\n",
        "\n",
        "\n",
        "class ModifiedTensorBoard(TensorBoard):\n",
        "\n",
        "    #By default Keras wants to create a new TensorBoard file after every fit\n",
        "    #But we want only a single log file, therefore, this class is created to solve this issue\n",
        "\n",
        "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.step = 1\n",
        "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
        "\n",
        "    # Overriding this method to stop creating default log writer\n",
        "    def set_model(self, model):\n",
        "        pass\n",
        "\n",
        "    # Overrided, saves logs with our step number\n",
        "    # (otherwise every .fit() will start writing from 0th step)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.update_stats(**logs)\n",
        "\n",
        "    # Overrided\n",
        "    # We train for one batch only, no need to save anything at epoch end\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        pass\n",
        "\n",
        "    # Overrided, so won't close writer\n",
        "    def on_train_end(self, _):\n",
        "        pass\n",
        "\n",
        "    # Custom method for saving own metrics\n",
        "    # Creates writer, writes custom metrics and closes writer\n",
        "    def update_stats(self, **stats):\n",
        "        self._write_logs(stats, self.step)\n",
        "\n",
        "class DQNAgent:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        #Main model\n",
        "        self.model=self.create_model()\n",
        "        #Target model\n",
        "        self.target_model=self.create_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        #Initially it will do just random exploration and eventually learn about the optimal value\n",
        "        #Therefore, not advisable to update weights after each predict\n",
        "\n",
        "        #Will fit main model that will fitted after every step (Trained every step)\n",
        "        #Target model will be the one we will do predict every step\n",
        "\n",
        "        #After some n number of epochs we set weights of Train model same as that of Main model\n",
        "        #Stablises the model, and a lot of randomness is noticed in initial steps\n",
        "\n",
        "        self.replay_memory= deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "        #List with a fixed max length , will store last maxlen number of steps of Main model\n",
        "\n",
        "        #Batch Learning generally makes a better and stabilised model (doesn't overfits)\n",
        "        #Now we take a random samle out of these 50000 memory and then this batch is what we feed to Target Model\n",
        "\n",
        "        self.tensorBoard= ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format(MODEL_NAME, int(time.time())))\n",
        "\n",
        "        self.target_update_counter= 0 # Will use to track and tell when to update the Target Network\n",
        "\n",
        "\n",
        "\n",
        "    def create_model():\n",
        "        model= Sequential()\n",
        "        model.add(Input(16,)) #We have to check about the input states (Observation states) #Number of max layers (in terms of layer id)\n",
        "        model.add(Dense(32,activation='relu'))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(128, activation='linear')) #Output  is number of Action state space - Number of possible actions, like vocab size\n",
        "\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics= ['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def update_replay_memory(self, transition):\n",
        "        self.replay_memory.append(transition)\n",
        "        #Transition is (Observation space , action,  reward,  new observatoin state)\n",
        "\n",
        "        #Main model get the q values of all possible actions for the current state\n",
        "    def get_qs(self, state, step):\n",
        "        return self.model.predict(state)[0]\n",
        "    #Return a one element array\n",
        "\n",
        "    #Only train when certain number of samples have been stord in the replay table\n",
        "    def train(self, terminal_state, step):\n",
        "        if len(self.replay_memory)< MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "\n",
        "        minibatch=random.sample(self.replay_memory,MINIBATCH_SIZE)\n",
        "\n",
        "\n",
        "        current_states= np.array([transition[0] for transition in minibatch])\n",
        "        current_qs_list= self.model.predict(current_states)\n",
        "\n",
        "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
        "\n",
        "        future_qs_list=self.target_model.predict(new_current_states)\n",
        "        #Need Q values for future current states in order to apply the formula for updation of Q values\n",
        "\n",
        "        X=[] #Images from gaem, what will be the input\n",
        "        y=[] #Action we take, what are the action, Input might be the model and output might be the accuracy predicted\n",
        "\n",
        "        #Done is whether we are done with the environment or not\n",
        "        #rest 3 are what is present in minibatch\n",
        "        #Used to caluclate the second half of the updation formula for Q-values\n",
        "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch) :\n",
        "            if not done:\n",
        "                max_future_q= np.max(future_qs_list[index])\n",
        "                new_q=reward+ DISCOUNT*max_future_q\n",
        "            if done :\n",
        "                new_q =reward\n",
        "\n",
        "            current_qs=current_qs_list[index]\n",
        "            current_qs[action]=new_q\n",
        "            X.append(current_state)\n",
        "            y.append(current_qs)\n",
        "\n",
        "        self.model.fit(np.array(X),np.array(y), batch_size= MINIBATCH_SIZE, verbose=0, shuffle=False, callbacks=[self.tensorBoard] if terminal_state else None)\n",
        "    #Output of the Neural Network is the Q-values, so in order to update the Q-value (max one) generated,\n",
        "    #we save the output, make changes in it, and then fit the neural network that it generates our specified values\n",
        "    #If on terminal state do fit else do nothing\n",
        "\n",
        "        #Updating thecounter and checking whether we want to update the model or not\n",
        "        if terminal_state:\n",
        "            self.target_update_counter+=1\n",
        "\n",
        "        if self.target_update_counter>UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.target_update_counter=0"
      ],
      "metadata": {
        "id": "1NaPhtvee947"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Mar 22 09:58:50 2022\n",
        "\n",
        "@author: AnshumaanChauhan\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten, Input, LSTM\n",
        "from keras.models import Model\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.activations import relu\n",
        "from keras.optimizers import *\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import tensorflow.keras.optimizers\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# import CNNCONSTANTS\n",
        "# from CNNCONSTANTS import *\n",
        "\n",
        "# from CNNGenerator import CNNSearchSpace\n",
        "\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "#In order to make results comparable for different models\n",
        "random.seed(1)\n",
        "\n",
        "\n",
        "class DQNAgent (CNNSearchSpace):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        REPLAY_MEMORY_SIZE = 50000 # Can also write as 50_000 for readability\n",
        "        MODEL_NAME= \"First_Try\"\n",
        "        self.MIN_REPLAY_MEMORY_SIZE=10\n",
        "        #MINIBATCH_SIZE= 64 #Usually the size of mini batch is 32, 64, or multiple of 8\n",
        "        #DISCOUNT= 0.99\n",
        "        self.UPDATE_TARGET_EVERY=3\n",
        "\n",
        "\n",
        "        #Constants for epsilon greedy algo\n",
        "        self.EPSILON=1 #Will be decayed over the training process\n",
        "        self.EPSILON_DECAY=0.01\n",
        "        self.seq_data=[]\n",
        "        self.replay_memory=[]\n",
        "        #self.replay_memory= deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "        #Will store x,y, val accuracy, pred_accuracy for all the things geenrated\n",
        "        #List with a fixed max length , will store last maxlen number of steps of Main model\n",
        "\n",
        "        #Batch Learning generally makes a better and stabilised model (doesn't overfits)\n",
        "        #Now we take a random samle out of these 50000 memory and then this batch is what we feed to Target Model\n",
        "\n",
        "        self.target_update_counter= 0 # Will use to track and tell when to update the Target Network\n",
        "\n",
        "        self.max_len = MAX_ARCHITECTURE_LENGTH\n",
        "        self.controller_lstm_dim = CONTROLLER_LSTM_DIM\n",
        "        self.controller_optimizer = CONTROLLER_OPTIMIZER\n",
        "        self.controller_lr = CONTROLLER_LEARNING_RATE\n",
        "        self.controller_decay = CONTROLLER_DECAY\n",
        "        self.controller_momentum = CONTROLLER_MOMENTUM\n",
        "        self.use_predictor = CONTROLLER_USE_PREDICTOR\n",
        "\n",
        "        # inheriting from the search space\n",
        "        super().__init__(TARGET_CLASSES)\n",
        "\n",
        "        # number of classes for the controller (+ 1 for padding)\n",
        "        self.controller_classes = len(self.vocab) + 1\n",
        "\n",
        "        # file path of controller weights to be stored at\n",
        "        self.controller_weights = 'LOGS2/controller_weights.h5'\n",
        "\n",
        "        #Main model\n",
        "        #self.model=self.create_control_model()\n",
        "        #Target model\n",
        "        #self.target_model=self.create_control_model()\n",
        "        #self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        #Initially it will do just random exploration and eventually learn about the optimal value\n",
        "        #Therefore, not advisable to update weights after each predict\n",
        "\n",
        "        #Will fit main model that will fitted after every step (Trained every step)\n",
        "        #Target model will be the one we will do predict every step\n",
        "\n",
        "        #After some n number of epochs we set weights of Train model same as that of Main model\n",
        "        #Stablises the model, and a lot of randomness is noticed in initial steps\n",
        "\n",
        "    def sample_architecture_sequences(self, model, number_of_samples):\n",
        "        # define values needed for sampling\n",
        "        final_layer_id = len(self.vocab)\n",
        "        BatchNorm_id = final_layer_id - 1\n",
        "        Flatten_id=final_layer_id-2\n",
        "        vocab_idx = [0] + list(self.vocab.keys())\n",
        "\n",
        "        # initialize list for architecture samples\n",
        "        samples = []\n",
        "        print(\"GENERATING ARCHITECTURE SAMPLES...\")\n",
        "        print('------------------------------------------------------')\n",
        "\n",
        "        # while number of architectures sampled is less than required\n",
        "        while len(samples) < number_of_samples:\n",
        "\n",
        "            # initialise the empty list for architecture sequence\n",
        "            seed = []\n",
        "\n",
        "            # while len of generated sequence is less than maximum architecture length\n",
        "            while len(seed) < self.max_len:\n",
        "\n",
        "                # pad sequence for correctly shaped input for controller\n",
        "                sequence = pad_sequences([seed], maxlen=self.max_len - 1, padding='post')\n",
        "                sequence = sequence.reshape(1, 1, self.max_len - 1)\n",
        "\n",
        "                # given the previous elements, get softmax distribution for the next element\n",
        "                if self.use_predictor:\n",
        "                    (probab, _) = model.predict(sequence)\n",
        "                else:\n",
        "                    probab = model.predict(sequence)\n",
        "                #print(probab[0])\n",
        "                #print(len(probab[0]))\n",
        "                #probab = probab[0][0]\n",
        "                #print(probab)\n",
        "                # sample the next element randomly given the probability of next elements (the softmax distribution)\n",
        "\n",
        "                '''\n",
        "                random_val=random.random()\n",
        "                if self.EPSILON > 0:\n",
        "                    if random_val<self.EPSILON:\n",
        "                        next = np.random.choice(vocab_idx, size=1, p=probab[0])\n",
        "                        next=next[0]\n",
        "                        self.EPSILON=self.EPSILON-self.EPSILON_DECAY\n",
        "                    else:\n",
        "                        best_action= max(probab[0])\n",
        "                        list_rep=probab[0].tolist()\n",
        "                        next=vocab_idx[list_rep.index(best_action)]\n",
        "                else:\n",
        "                    best_action= max(probab[0])\n",
        "                    list_rep=probab[0].tolist()\n",
        "                    next=vocab_idx[list_rep.index(best_action)]\n",
        "                '''\n",
        "\n",
        "                next = np.random.choice(vocab_idx, size=1, p=probab[0])\n",
        "                #Here we have to specify a range of values, to cover the point of dropout cannot be the first layer\n",
        "                if (next >= self.conv_id) and len(seed) == 0:\n",
        "                    continue\n",
        "                #Have to make a rule such that first layer cannot be anything except the Convolutional Layer\n",
        "\n",
        "                # first layer is not final layer\n",
        "                if next == final_layer_id and len(seed) == 0:\n",
        "                    continue\n",
        "\n",
        "                # if final layer, break out of inner loop\n",
        "                if next == final_layer_id:\n",
        "                    seed.pop()\n",
        "                    seed.append(Flatten_id)\n",
        "                    seed.append(next)\n",
        "                    break\n",
        "\n",
        "                # if sequence length is 1 less than maximum, add final\n",
        "                # layer and break out of inner loop\n",
        "                if len(seed) == self.max_len - 2:\n",
        "                    seed.append(Flatten_id)\n",
        "                    seed.append(final_layer_id)\n",
        "                    break\n",
        "\n",
        "                # ignore padding\n",
        "                if not next == 0:\n",
        "                    check_insert=False\n",
        "                    if next > self.pool_id and next <= self.fully_id :\n",
        "                        for i in seed:\n",
        "                            if i == Flatten_id :\n",
        "                                seed.append(next)\n",
        "                                check_insert=True\n",
        "                                break\n",
        "\n",
        "                    if next == Flatten_id :\n",
        "                        check_dupli_flatten=False\n",
        "                        for i in seed :\n",
        "                            if i==next:\n",
        "                                check_dupli_flatten=True\n",
        "                        if not check_dupli_flatten:\n",
        "                            seed.append(next)\n",
        "                            check_insert=True\n",
        "\n",
        "                    if next <= self.pool_id :\n",
        "                        check_no_pool_conv_after_flatten=False\n",
        "                        for i in seed:\n",
        "                            if i == Flatten_id:\n",
        "                                check_no_pool_conv_after_flatten=True\n",
        "                        if not check_no_pool_conv_after_flatten :\n",
        "                            seed.append(next)\n",
        "                            check_insert=True\n",
        "\n",
        "                    if next > self.fully_id and next<=self.reg_layer_id:\n",
        "                        i=seed[-1]\n",
        "                        if ((i>self.conv_id and i<=self.pool_id) or (i>self.pool_id and i<=self.fully_id)):\n",
        "                            seed.append(next)\n",
        "                            check_insert=True\n",
        "\n",
        "                    if next== BatchNorm_id:\n",
        "                        i=seed[-1]\n",
        "                        if i>self.fully_id and i<=self.reg_layer_id:\n",
        "                            seed.append(next)\n",
        "                            check_insert=True\n",
        "\n",
        "                    if not check_insert:\n",
        "                        seed.append(next)\n",
        "                else:\n",
        "                    continue\n",
        "            # check if the generated sequence has been generated before.\n",
        "            # if not, add it to the sequence data.\n",
        "            if seed not in self.seq_data:\n",
        "                samples.append(seed)\n",
        "                self.seq_data.append(seed)\n",
        "        return samples\n",
        "\n",
        "    def create_control_model(self, controller_input_shape, controller_batch_size):\n",
        "\n",
        "        main_input=Input(shape=controller_input_shape, name='main_input')\n",
        "        x= LSTM(self.controller_lstm_dim, return_sequences=True)(main_input)\n",
        "        x2=Dropout(0.2)(x)\n",
        "        x3=LSTM(self.controller_lstm_dim)(x2)\n",
        "        x4=Dropout(0.2)(x3)\n",
        "        main_output=Dense(self.controller_classes, activation='softmax', name='main_output')(x4)\n",
        "        model=Model(inputs=[main_input],outputs=[main_output])\n",
        "        return model\n",
        "\n",
        "\n",
        "    def create_hybrid_model(self, controller_input_shape, controller_batch_size):\n",
        "\n",
        "        main_input=Input(shape=controller_input_shape, name='main_input')\n",
        "        x= LSTM(self.controller_lstm_dim, return_sequences=True)(main_input)\n",
        "        x2=Dropout(0.2)(x)\n",
        "        x3=LSTM(self.controller_lstm_dim)(x2)\n",
        "        x4=Dropout(0.2)(x3)\n",
        "        main_output=Dense(self.controller_classes, activation='softmax', name='main_output')(x4)\n",
        "\n",
        "        # LSTM layer\n",
        "        x5 = LSTM(self.controller_lstm_dim, return_sequences=True)(main_input)\n",
        "        # single neuron sigmoid layer for accuracy prediction\n",
        "        predictor_output = Dense(1, activation='sigmoid', name='predictor_output')(x2)\n",
        "\n",
        "        # finally the Keras Model class is used to create a multi-output model\n",
        "        model = Model(inputs=[main_input], outputs=[main_output, predictor_output])\n",
        "        return model\n",
        "\n",
        "\n",
        "    #Only train when certain number of samples have been stord in the replay table\n",
        "    def train_control_model(self, model, target_model, x_data, y_data, val_accuracy, loss_func, controller_batch_size, nb_epochs):\n",
        "\n",
        "\n",
        "        for i in range(len(x_data)):\n",
        "            self.replay_memory.append([x_data[i][0],y_data[i],val_accuracy[i]])\n",
        "\n",
        "        if len(self.replay_memory)<self.MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "        #Top 250 Architectures are taken\n",
        "        self.replay_memory= sorted(self.replay_memory,key=itemgetter(2))\n",
        "        to_train=self.replay_memory[:1]\n",
        "        optim = getattr(tensorflow.keras.optimizers, self.controller_optimizer)(lr=self.controller_lr,\n",
        "                                                       decay=self.controller_decay)\n",
        "\n",
        "        # compile model depending on loss function and optimizer provided\n",
        "        model.compile(optimizer=optim, loss={'main_output': loss_func})\n",
        "\n",
        "        # load controller weights\n",
        "        if os.path.exists(self.controller_weights):\n",
        "            model.load_weights(self.controller_weights)\n",
        "\n",
        "        x_data=[]\n",
        "        y_data=[]\n",
        "        for i in range(len(to_train)):\n",
        "            x_data.append(to_train[i][0].reshape(1,7))\n",
        "            y_data.append(to_train[i][1])\n",
        "\n",
        "        # train the controller\n",
        "\n",
        "        #We are trying to make it learn that if the previous layers are given in this order and the next predicted is final\n",
        "        #Taking the ones with best accuracy helps it learn which is better\n",
        "        print(\"TRAINING CONTROLLER...\")\n",
        "        model.fit({'main_input': np.array(x_data)},\n",
        "                  {'main_output': np.array(y_data)},\n",
        "                  epochs=nb_epochs,\n",
        "                  batch_size=controller_batch_size,\n",
        "                  verbose=0)\n",
        "        #{'main_output': y_data.reshape(len(y_data), 1, self.controller_classes)}\n",
        "        # save controller weights\n",
        "        model.save_weights(self.controller_weights)\n",
        "\n",
        "        #Updating the counter and checking whether we want to update the model or not\n",
        "        self.target_update_counter+=1\n",
        "\n",
        "        #If we are at the point where counter value id reached, then we just copy the weights from Main model to Target Model\n",
        "        if self.target_update_counter>self.UPDATE_TARGET_EVERY:\n",
        "            print(\"TRANSFERRING WEIGHTS...\")\n",
        "            target_model.set_weights(model.get_weights())\n",
        "            #Reinitialize target update counter value to 0\n",
        "            self.target_update_counter=0\n",
        "\n",
        "\n",
        "\n",
        "    def train_hybrid_model(self, model, target_model, x_data, y_data, val_accuracy, pred_accuracy, loss_func, controller_batch_size, nb_epochs):\n",
        "\n",
        "        for i in range(len(x_data)):\n",
        "            self.replay_memory.append([x_data[i][0],y_data[i],val_accuracy[i],pred_accuracy[i]])\n",
        "\n",
        "        if len(self.replay_memory)<self.MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "        #Top 250 Architectures are taken\n",
        "        self.replay_memory= sorted(self.replay_memory,key=itemgetter(2))\n",
        "        to_train=self.replay_memory[:1]\n",
        "        optim = getattr(tensorflow.keras.optimizers, self.controller_optimizer)(lr=self.controller_lr, decay=self.controller_decay, clipnorm=1.0)\n",
        "\n",
        "\n",
        "        model.compile(optimizer=optim,\n",
        "                      loss={'main_output': loss_func, 'predictor_output': 'mse'},\n",
        "                      loss_weights={'main_output': 1, 'predictor_output': 1})\n",
        "\n",
        "        if os.path.exists(self.controller_weights):\n",
        "            model.load_weights(self.controller_weights)\n",
        "\n",
        "        x_data=[]\n",
        "        y_data=[]\n",
        "        pred_target=[]\n",
        "        for i in range(len(to_train)):\n",
        "            x_data.append(to_train[i][0].reshape(1,7))\n",
        "            y_data.append(to_train[i][1])\n",
        "            pred_target.append(to_train[i][3])\n",
        "        print(\"TRAINING CONTROLLER...\")\n",
        "        model.fit({'main_input': np.array(x_data)},\n",
        "                  {'main_output': np.array(y_data),\n",
        "                   'predictor_output': np.array(pred_target)},\n",
        "                  epochs=nb_epochs,\n",
        "                  batch_size=controller_batch_size,\n",
        "                  verbose=0)\n",
        "\n",
        "        model.save_weights(self.controller_weights)\n",
        "\n",
        "        self.target_update_counter+=1\n",
        "\n",
        "        #If we are at the point where counter value id reached, then we just copy the weights from Main model to Target Model\n",
        "        if self.target_update_counter>self.UPDATE_TARGET_EVERY:\n",
        "            print(\"TRANSFERRING WEIGHTS...\")\n",
        "            self.target_model.set_weights(model.get_weights())\n",
        "            #Reinitialize target update counter value to 0\n",
        "            self.target_update_counter=0\n",
        "\n",
        "\n",
        "\n",
        "    def get_predicted_accuracies_hybrid_model(self, model, seqs):\n",
        "        pred_accuracies = []\n",
        "        for seq in seqs:\n",
        "            # pad each sequence\n",
        "            control_sequences = pad_sequences([seq], maxlen=self.max_len, padding='post')\n",
        "            xc = control_sequences[:, :-1].reshape(len(control_sequences), 1, self.max_len - 1)\n",
        "            # get predicted accuracies\n",
        "            (_, pred_accuracy) = [x[0][0] for x in model.predict(xc)]\n",
        "            pred_accuracies.append(pred_accuracy[0])\n",
        "        return pred_accuracies"
      ],
      "metadata": {
        "id": "GoJsjhnSffDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Apr 23 18:09:06 2022\n",
        "\n",
        "@author: AnshumaanChauhan\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Feb 22 09:58:43 2022\n",
        "\n",
        "@author: AnshumaanChauhan\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pickle\n",
        "import numpy as np\n",
        "from itertools import groupby\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# from CNNCONSTANTS import *\n",
        "# from CNNGenerator import CNNSearchSpace\n",
        "\n",
        "\n",
        "########################################################\n",
        "#                   DATA PROCESSING                    #\n",
        "########################################################\n",
        "\n",
        "\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "\n",
        "########################################################\n",
        "#                       LOGGING                        #\n",
        "########################################################\n",
        "\n",
        "\n",
        "def clean_log():\n",
        "    filelist = os.listdir('LOGS2')\n",
        "    for file in filelist:\n",
        "        if os.path.isfile('LOGS2/{}'.format(file)):\n",
        "            os.remove('LOGS2/{}'.format(file))\n",
        "\n",
        "\n",
        "def log_event():\n",
        "    dest = 'LOGS'\n",
        "    while os.path.exists(dest):\n",
        "        dest = 'LOGS2/event{}'.format(np.random.randint(10000))\n",
        "    os.mkdir(dest)\n",
        "    filelist = os.listdir('LOGS2')\n",
        "    for file in filelist:\n",
        "        if os.path.isfile('LOGS2/{}'.format(file)):\n",
        "            shutil.move('LOGS2/{}'.format(file),dest)\n",
        "\n",
        "\n",
        "def get_latest_event_id():\n",
        "    all_subdirs = ['LOGS2/' + d for d in os.listdir('LOGS2') if os.path.isdir('LOGS2/' + d)]\n",
        "    latest_subdir = max(all_subdirs, key=os.path.getmtime)\n",
        "    return int(latest_subdir.replace('LOGS2/event', ''))\n",
        "\n",
        "\n",
        "########################################################\n",
        "#                 RESULTS PROCESSING                   #\n",
        "########################################################\n",
        "\n",
        "\n",
        "def load_nas_data():\n",
        "    event = get_latest_event_id()\n",
        "    data_file = 'LOGS2/event{}/nas_data.pkl'.format(event)\n",
        "    with open(data_file, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def sort_search_data(nas_data):\n",
        "    val_accs = [item[1] for item in nas_data]\n",
        "    sorted_idx = np.argsort(val_accs)[::-1]\n",
        "    nas_data = [nas_data[x] for x in sorted_idx]\n",
        "    return nas_data\n",
        "\n",
        "########################################################\n",
        "#                EVALUATION AND PLOTS                  #\n",
        "########################################################\n",
        "\n",
        "def get_top_n_architectures(n):\n",
        "    data = load_nas_data()\n",
        "    data = sort_search_data(data)\n",
        "    search_space = CNNSearchSpace(TARGET_CLASSES)\n",
        "    print('Top {} Architectures:'.format(n))\n",
        "    for seq_data in data[:n]:\n",
        "        print('Architecture', search_space.decode_sequence(seq_data[0]))\n",
        "        print('Validation Accuracy:', seq_data[1])\n",
        "\n",
        "\n",
        "def get_nas_accuracy_plot():\n",
        "    data = load_nas_data()\n",
        "    accuracies = [x[1] for x in data]\n",
        "    plt.plot(np.arange(len(data)), accuracies)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_accuracy_distribution():\n",
        "    event = get_latest_event_id()\n",
        "    data = load_nas_data()\n",
        "    accuracies = [x[1]*100. for x in data]\n",
        "    accuracies = [int(x) for x in accuracies]\n",
        "    sorted_accs = np.sort(accuracies)\n",
        "    count_dict = {k: len(list(v)) for k, v in groupby(sorted_accs)}\n",
        "    plt.bar(list(count_dict.keys()), list(count_dict.values()))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OS0UBBx5gRIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Mar 22 14:41:33 2022\n",
        "\n",
        "@author: AnshumaanChauhan\n",
        "\"\"\"\n",
        "\n",
        "# import CNNCONSTANTS\n",
        "import pickle\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "# from NASutils import *\n",
        "# from CNNCONSTANTS import *\n",
        "# from DQNController import DQNAgent\n",
        "# from CNNGenerator import CNNGenerator\n",
        "class CNNNAS(DQNAgent):\n",
        "\n",
        "\n",
        "    def __init__(self, x, y):\n",
        "\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.target_classes = TARGET_CLASSES\n",
        "        self.controller_sampling_epochs = CONTROLLER_SAMPLING_EPOCHS\n",
        "        self.samples_per_controller_epoch = SAMPLES_PER_CONTROLLER_EPOCH\n",
        "        self.controller_train_epochs = CONTROLLER_TRAINING_EPOCHS\n",
        "        self.architecture_train_epochs = ARCHITECTURE_TRAINING_EPOCHS\n",
        "        self.controller_loss_alpha = CONTROLLER_LOSS_ALPHA\n",
        "\n",
        "        self.data = []\n",
        "        self.nas_data_log = 'LOGS2/nas_data.pkl'\n",
        "        clean_log()\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.CNNGenerator = CNNGenerator()\n",
        "\n",
        "        self.controller_batch_size = len(self.data)\n",
        "        self.controller_input_shape = (1, MAX_ARCHITECTURE_LENGTH - 1)\n",
        "\n",
        "        if self.use_predictor:\n",
        "            self.controller_model = self.create_hybrid_model(self.controller_input_shape, self.controller_batch_size)\n",
        "            self.target_model= self.create_hybrid_model(self.controller_input_shape, self.controller_batch_size)\n",
        "        else:\n",
        "            self.controller_model = self.create_control_model(self.controller_input_shape, self.controller_batch_size)\n",
        "            self.target_model= self.create_control_model(self.controller_input_shape, self.controller_batch_size)\n",
        "\n",
        "    def create_architecture(self, sequence):\n",
        "        if self.target_classes == 2:\n",
        "            self.CNNGenerator.loss_func = 'binary_crossentropy'\n",
        "        model = self.CNNGenerator.create_model(sequence, np.shape(self.x[0]))\n",
        "        #models = self.CNNGenerator.compile_model(model)\n",
        "        if model==None:\n",
        "            return model\n",
        "        models = self.CNNGenerator.compile_model(model)\n",
        "        return models\n",
        "\n",
        "    def train_architecture(self, model):\n",
        "        x, y = unison_shuffled_copies(self.x, self.y)\n",
        "        #Check how to train models on different number of epochs\n",
        "        history_of_models = self.CNNGenerator.train_model(model, x, y, self.architecture_train_epochs)\n",
        "        return history_of_models\n",
        "\n",
        "    def append_model_metrics(self, sequence, history, pred_accuracy=None):\n",
        "        if len(history.history['val_accuracy']) == 1:\n",
        "            if pred_accuracy:\n",
        "                self.data.append([sequence,\n",
        "                                  history.history['val_accuracy'][0],\n",
        "                                  pred_accuracy])\n",
        "                print('predicted accuracy: ',pred_accuracy)\n",
        "            else:\n",
        "                self.data.append([sequence,\n",
        "                                  history.history['val_accuracy'][0]])\n",
        "            print('validation accuracy: ', history.history['val_accuracy'][0])\n",
        "        else:\n",
        "            val_acc = np.ma.average(history.history['val_accuracy'],\n",
        "                                    weights=np.arange(1, len(history.history['val_accuracy']) + 1),\n",
        "                                    axis=-1)\n",
        "            if pred_accuracy:\n",
        "                self.data.append([sequence,\n",
        "                                  val_acc,\n",
        "                                  pred_accuracy])\n",
        "            else:\n",
        "                self.data.append([sequence,\n",
        "                                  val_acc])\n",
        "            print('validation accuracy: ', val_acc)\n",
        "\n",
        "    def prepare_controller_data(self, sequences):\n",
        "        #Adds 0 at the end if the sequence length is shorter than Max length architecture\n",
        "        controller_sequences = pad_sequences(sequences, maxlen=self.max_len, padding='post')\n",
        "        #Have all the layers except the final softmax layer\n",
        "        xc = controller_sequences[:, :-1].reshape(len(controller_sequences), 1, self.max_len - 1)\n",
        "        #Final layer\n",
        "        yc = to_categorical(controller_sequences[:, -1], self.controller_classes)\n",
        "        #Getting val accuracy of the sequences\n",
        "        val_acc_target = [item[1] for item in self.data]\n",
        "        return xc, yc, val_acc_target\n",
        "\n",
        "    def get_discounted_reward(self, rewards):\n",
        "        discounted_r = np.zeros_like(rewards, dtype=np.float32)\n",
        "        for t in range(len(rewards)):\n",
        "            running_add = 0.\n",
        "            exp = 0.\n",
        "            for r in rewards[t:]:\n",
        "                running_add += self.controller_loss_alpha**exp * r\n",
        "                exp += 1\n",
        "            discounted_r[t] = running_add\n",
        "        discounted_r = (discounted_r - discounted_r.mean()) / discounted_r.std()\n",
        "        return discounted_r\n",
        "\n",
        "    def custom_loss(self, target, output):\n",
        "        baseline = 0.5\n",
        "        reward = np.array([item[1] - baseline for item in self.data[-self.samples_per_controller_epoch:]]).reshape(\n",
        "            self.samples_per_controller_epoch, 1)\n",
        "        discounted_reward = self.get_discounted_reward(reward)\n",
        "        loss = - K.log(output) * discounted_reward[:, None]\n",
        "        return loss\n",
        "\n",
        "    def train_controller(self, model, x, y, val_accuracy, pred_accuracy=None):\n",
        "        if self.use_predictor:\n",
        "            self.train_hybrid_model(model,\n",
        "                                    self.target_model,\n",
        "                                    x,\n",
        "                                    y,\n",
        "                                    val_accuracy,\n",
        "                                    pred_accuracy,\n",
        "                                    self.custom_loss,\n",
        "                                    len(self.data),\n",
        "                                    self.controller_train_epochs)\n",
        "        else:\n",
        "            self.train_control_model(model,\n",
        "                                     self.target_model,\n",
        "                                     x,\n",
        "                                     y,\n",
        "                                     val_accuracy,\n",
        "                                     self.custom_loss,\n",
        "                                     len(self.data),\n",
        "                                     self.controller_train_epochs)\n",
        "\n",
        "    def search(self):\n",
        "        for controller_epoch in range(self.controller_sampling_epochs):\n",
        "            print('------------------------------------------------------------------')\n",
        "            print('                       CONTROLLER EPOCH: {}'.format(controller_epoch))\n",
        "            print('------------------------------------------------------------------')\n",
        "            sequences = self.sample_architecture_sequences(self.controller_model, self.samples_per_controller_epoch)\n",
        "            if self.use_predictor:\n",
        "                pred_accuracies = self.get_predicted_accuracies_hybrid_model(self.controller_model, sequences)\n",
        "                #print(\"At start print acc: \",pred_accuracies)\n",
        "            for i, sequence in enumerate(sequences):\n",
        "                print('Architecture: ', self.decode_sequence(sequence))\n",
        "                model = self.create_architecture(sequence)\n",
        "                if model==None:\n",
        "                    if self.use_predictor:\n",
        "                        self.data.append([sequence, -10.0, pred_accuracies[i]])\n",
        "                        print('validation accuracy: ', -10.0)\n",
        "                    else:\n",
        "                        self.data.append([sequence, -10.0])\n",
        "                        print('validation accuracy: ', -10.0)\n",
        "                    continue\n",
        "                history = self.train_architecture(model)\n",
        "                if self.use_predictor:\n",
        "                    self.append_model_metrics(sequence, history, pred_accuracies[i])\n",
        "                else:\n",
        "                    self.append_model_metrics(sequence, history)\n",
        "                print('------------------------------------------------------')\n",
        "            xc, yc, val_acc_target = self.prepare_controller_data(sequences)\n",
        "            if self.use_predictor:\n",
        "                self.train_controller(self.controller_model,\n",
        "                                  xc,\n",
        "                                  yc,\n",
        "                                  val_acc_target[-self.samples_per_controller_epoch:], pred_accuracies)\n",
        "            else:\n",
        "                self.train_controller(self.controller_model,\n",
        "                                  xc,\n",
        "                                  yc,\n",
        "                                  val_acc_target[-self.samples_per_controller_epoch:])\n",
        "        with open(self.nas_data_log, 'wb') as f:\n",
        "            pickle.dump(self.data, f)\n",
        "        log_event()\n",
        "        return self.data"
      ],
      "metadata": {
        "id": "WKbvrvF0gZVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def clean_log():\n",
        "    log_directory = 'LOGS2'\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    if not os.path.exists(log_directory):\n",
        "        os.makedirs(log_directory)\n",
        "\n",
        "    filelist = os.listdir(log_directory)\n",
        "    for file in filelist:\n",
        "        if os.path.isfile(os.path.join(log_directory, file)):\n",
        "            os.remove(os.path.join(log_directory, file))"
      ],
      "metadata": {
        "id": "PC7L1h3rh7aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Mar 22 14:40:18 2022\n",
        "\n",
        "@author: AnshumaanChauhan\n",
        "\"\"\"\n",
        "\n",
        "# import CNNCONSTANTS\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# from NASutils import *\n",
        "# from cnnnas import CNNNAS\n",
        "# from CNNCONSTANTS import TOP_N\n",
        "from keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "#Use MNIST for the time being\n",
        "#data = pd.read_csv('DATASETS/wine-quality.csv')\n",
        "#x = data.drop('quality_label', axis=1, inplace=False).values\n",
        "#y = pd.get_dummies(data['quality_label']).values\n",
        "\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))\n",
        "train_y=to_categorical(train_y,num_classes=10)\n",
        "train_new_X=train_X[:2]\n",
        "train_new_y=train_y[:2]\n",
        "nas_object = CNNNAS(train_new_X, train_new_y)\n",
        "data = nas_object.search()\n",
        "get_top_n_architectures(TOP_N)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E9Up0ViGgBk2",
        "outputId": "755a3eef-4e5d-4163-9874-5c464a3c47b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing shared weights dictionary...\n",
            "------------------------------------------------------------------\n",
            "                       CONTROLLER EPOCH: 0\n",
            "------------------------------------------------------------------\n",
            "GENERATING ARCHITECTURE SAMPLES...\n",
            "------------------------------------------------------\n",
            "1/1 [==============================] - 1s 784ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Architecture:  [('conv2d', 128, 7, 3, 'same', 'RandomUniform', 'RandomUniform', 'l2'), ('conv2d', 160, 5, 2, 'same', 'HeUniform', 'RandomNormal', 'l1'), ('separableconv2d', 64, 7, 2, 'valid', 'HeNormal', 'RandomNormal', 'l1_l2'), ('separableconv2d', 224, 5, 3, 'same', 'HeUniform', 'RandomNormal', 'l2'), ('conv2d', 224, 5, 2, 'same', 'RandomNormal', 'HeNormal', 'l2'), ('conv2d', 256, 5, 3, 'same', 'RandomUniform', 'RandomNormal', 'l1'), 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  -10.0\n",
            "Architecture:  [('depthwiseconv2d', 3, 3, 'same', 'RandomUniform', 'RandomNormal', 'l1_l2'), ('separableconv2d', 128, 7, 2, 'valid', 'RandomNormal', 'RandomNormal', 'l1_l2'), ('conv2dtranspose', 32, 5, 3, 'same', 'RandomUniform', 'HeNormal', 'l1'), ('conv2d', 128, 9, 2, 'valid', 'HeNormal', 'HeUniform', 'l2'), ('separableconv2d', 96, 9, 3, 'same', 'RandomNormal', 'HeNormal', 'l2'), 'globalavgpool2d', 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  -10.0\n",
            "Architecture:  [('depthwiseconv2d', 7, 3, 'valid', 'HeUniform', 'HeUniform', 'l1'), ('conv2dtranspose', 160, 9, 2, 'same', 'HeUniform', 'RandomUniform', 'l1'), ('conv2d', 224, 3, 3, 'same', 'RandomUniform', 'RandomNormal', 'l1_l2'), ('depthwiseconv2d', 9, 2, 'valid', 'HeUniform', 'HeUniform', 'l1_l2'), ('depthwiseconv2d', 3, 3, 'valid', 'RandomNormal', 'RandomNormal', 'l1_l2'), ('conv2dtranspose', 256, 3, 3, 'same', 'RandomNormal', 'RandomNormal', 'l2'), 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  -10.0\n",
            "Architecture:  [('conv2dtranspose', 192, 9, 3, 'same', 'HeNormal', 'HeNormal', 'l1'), ('conv2d', 192, 3, 3, 'valid', 'RandomNormal', 'RandomUniform', 'l1'), ('depthwiseconv2d', 9, 3, 'same', 'RandomUniform', 'HeNormal', 'l2'), ('conv2dtranspose', 128, 3, 3, 'valid', 'RandomUniform', 'RandomNormal', 'l1_l2'), ('separableconv2d', 160, 7, 3, 'same', 'HeUniform', 'RandomNormal', 'l1'), ('depthwiseconv2d', 7, 3, 'same', 'RandomUniform', 'RandomNormal', 'l1'), 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  0.0\n",
            "------------------------------------------------------\n",
            "Architecture:  [('conv2d', 96, 7, 3, 'same', 'HeNormal', 'HeNormal', 'l1'), ('depthwiseconv2d', 3, 2, 'same', 'RandomUniform', 'RandomNormal', 'l2'), 'globalavgpool2d', ('separableconv2d', 128, 3, 3, 'valid', 'RandomNormal', 'HeNormal', 'l1_l2'), ('conv2dtranspose', 64, 9, 2, 'same', 'RandomUniform', 'RandomNormal', 'l2'), ('depthwiseconv2d', 3, 2, 'valid', 'HeNormal', 'HeNormal', 'l2'), 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  -10.0\n",
            "Architecture:  [('depthwiseconv2d', 7, 2, 'valid', 'RandomNormal', 'RandomNormal', 'l1'), ('depthwiseconv2d', 7, 2, 'same', 'HeNormal', 'HeUniform', 'l1_l2'), ('depthwiseconv2d', 9, 3, 'same', 'HeUniform', 'HeUniform', 'l1_l2'), ('separableconv2d', 16, 3, 3, 'same', 'RandomUniform', 'HeUniform', 'l1'), ('conv2d', 16, 9, 2, 'same', 'RandomNormal', 'RandomUniform', 'l1'), ('conv2d', 64, 9, 3, 'valid', 'RandomUniform', 'RandomNormal', 'l1'), 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  -10.0\n",
            "Architecture:  [('conv2d', 192, 9, 2, 'same', 'HeNormal', 'HeUniform', 'l1'), ('conv2d', 128, 5, 2, 'valid', 'RandomNormal', 'HeNormal', 'l2'), ('depthwiseconv2d', 7, 3, 'valid', 'HeNormal', 'RandomNormal', 'l2'), ('conv2dtranspose', 224, 9, 2, 'valid', 'RandomNormal', 'HeUniform', 'l2'), ('separableconv2d', 192, 9, 3, 'valid', 'HeUniform', 'RandomUniform', 'l1_l2'), ('conv2d', 224, 7, 2, 'same', 'HeNormal', 'HeNormal', 'l1_l2'), 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  -10.0\n",
            "Architecture:  [('conv2dtranspose', 128, 5, 2, 'same', 'RandomUniform', 'HeUniform', 'l1'), ('conv2dtranspose', 160, 9, 2, 'same', 'HeUniform', 'HeNormal', 'l2'), ('conv2d', 160, 7, 2, 'same', 'RandomNormal', 'RandomNormal', 'l1_l2'), ('depthwiseconv2d', 7, 3, 'same', 'RandomUniform', 'RandomNormal', 'l1_l2'), ('conv2dtranspose', 32, 9, 2, 'same', 'HeUniform', 'HeUniform', 'l1_l2'), ('depthwiseconv2d', 3, 2, 'same', 'RandomNormal', 'RandomUniform', 'l1_l2'), 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  1.0\n",
            "------------------------------------------------------\n",
            "Architecture:  [('separableconv2d', 192, 5, 3, 'same', 'HeNormal', 'HeNormal', 'l1_l2'), ('depthwiseconv2d', 3, 3, 'same', 'HeUniform', 'HeUniform', 'l1'), ('separableconv2d', 64, 7, 2, 'valid', 'HeNormal', 'RandomUniform', 'l1'), ('depthwiseconv2d', 5, 2, 'valid', 'RandomUniform', 'HeUniform', 'l2'), ('conv2dtranspose', 16, 5, 3, 'valid', 'RandomUniform', 'RandomUniform', 'l1'), ('depthwiseconv2d', 5, 3, 'same', 'HeNormal', 'RandomNormal', 'l2'), 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  -10.0\n",
            "Architecture:  [('separableconv2d', 16, 3, 2, 'same', 'HeUniform', 'RandomNormal', 'l2'), ('separableconv2d', 32, 9, 3, 'valid', 'RandomNormal', 'RandomUniform', 'l1_l2'), ('depthwiseconv2d', 7, 2, 'same', 'RandomNormal', 'HeNormal', 'l2'), ('separableconv2d', 128, 3, 3, 'same', 'RandomUniform', 'HeNormal', 'l1_l2'), ('conv2dtranspose', 128, 9, 3, 'same', 'HeUniform', 'RandomNormal', 'l2'), ('conv2d', 64, 5, 3, 'same', 'HeUniform', 'RandomUniform', 'l2'), 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  0.0\n",
            "------------------------------------------------------\n",
            "Architecture:  [('separableconv2d', 160, 9, 2, 'same', 'RandomUniform', 'HeUniform', 'l1_l2'), ('depthwiseconv2d', 9, 3, 'same', 'RandomNormal', 'HeNormal', 'l1_l2'), ('conv2dtranspose', 256, 9, 3, 'same', 'RandomUniform', 'HeUniform', 'l1'), ('depthwiseconv2d', 7, 3, 'same', 'RandomNormal', 'RandomNormal', 'l1_l2'), ('conv2d', 160, 7, 2, 'valid', 'HeUniform', 'RandomNormal', 'l2'), ('conv2d', 160, 7, 2, 'valid', 'HeUniform', 'RandomUniform', 'l1'), 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  -10.0\n",
            "Architecture:  [('separableconv2d', 256, 3, 3, 'same', 'HeUniform', 'HeNormal', 'l1'), ('conv2dtranspose', 96, 9, 3, 'same', 'HeNormal', 'HeUniform', 'l1'), ('conv2d', 32, 5, 3, 'same', 'HeUniform', 'RandomUniform', 'l1_l2'), ('conv2dtranspose', 96, 5, 2, 'same', 'HeUniform', 'RandomNormal', 'l1'), ('depthwiseconv2d', 9, 3, 'valid', 'HeUniform', 'RandomNormal', 'l2'), ('separableconv2d', 224, 9, 2, 'valid', 'RandomUniform', 'HeUniform', 'l1'), 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  -10.0\n",
            "Architecture:  [('separableconv2d', 32, 9, 3, 'valid', 'HeUniform', 'RandomNormal', 'l1_l2'), ('depthwiseconv2d', 9, 2, 'same', 'HeUniform', 'RandomUniform', 'l1_l2'), ('separableconv2d', 192, 9, 2, 'same', 'RandomNormal', 'RandomUniform', 'l1_l2'), ('conv2d', 128, 7, 2, 'valid', 'RandomNormal', 'RandomUniform', 'l2'), ('separableconv2d', 128, 5, 3, 'same', 'HeNormal', 'HeUniform', 'l2'), ('depthwiseconv2d', 5, 2, 'same', 'HeNormal', 'RandomUniform', 'l1'), 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  -10.0\n",
            "Architecture:  [('conv2d', 96, 7, 2, 'valid', 'RandomNormal', 'RandomNormal', 'l1_l2'), ('depthwiseconv2d', 3, 3, 'same', 'RandomNormal', 'RandomUniform', 'l2'), ('separableconv2d', 128, 3, 2, 'valid', 'HeNormal', 'RandomUniform', 'l1_l2'), ('depthwiseconv2d', 5, 3, 'valid', 'RandomNormal', 'RandomNormal', 'l2'), ('separableconv2d', 128, 7, 3, 'valid', 'RandomUniform', 'HeUniform', 'l1'), ('depthwiseconv2d', 9, 3, 'same', 'HeUniform', 'HeUniform', 'l2'), 'Flatten', (10, 'softmax')]\n",
            "validation accuracy:  -10.0\n",
            "Architecture:  [('conv2dtranspose', 192, 9, 2, 'same', 'HeNormal', 'HeNormal', 'l1'), ('depthwiseconv2d', 9, 3, 'valid', 'RandomNormal', 'RandomUniform', 'l2'), ('separableconv2d', 224, 3, 2, 'valid', 'RandomNormal', 'HeUniform', 'l1_l2'), ('depthwiseconv2d', 3, 2, 'valid', 'HeNormal', 'HeNormal', 'l2'), ('depthwiseconv2d', 5, 3, 'same', 'HeUniform', 'HeUniform', 'l1_l2'), ('conv2dtranspose', 128, 9, 3, 'same', 'HeUniform', 'HeNormal', 'l2'), 'Flatten', (10, 'softmax')]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-45bae67c34ea>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtrain_new_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mnas_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNNAS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_new_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_new_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnas_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mget_top_n_architectures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTOP_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-85e12ccd4bf4>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_architecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_predictor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_model_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_accuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-85e12ccd4bf4>\u001b[0m in \u001b[0;36mtrain_architecture\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munison_shuffled_copies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m#Check how to train models on different number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mhistory_of_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCNNGenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marchitecture_train_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory_of_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-c494d326b09a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, models, x_data, y_data, nb_epochs, validation_split, callbacks)\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch_size_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                     history = model.fit(x_data,\n\u001b[0m\u001b[1;32m    463\u001b[0m                                 \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1854\u001b[0m                             \u001b[0mpss_evaluation_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pss_evaluation_shards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m                         )\n\u001b[0;32m-> 1856\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1857\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2294\u001b[0m                         ):\n\u001b[1;32m   2295\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2296\u001b[0;31m                             logs = test_function_runner.run_step(\n\u001b[0m\u001b[1;32m   2297\u001b[0m                                 \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m                                 \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   4106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m         \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    878\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}