{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning model"
      ],
      "metadata": {
        "id": "ISO1ExymL_b0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0cIWlPBrXbo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchdata\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhtrwYrOJMJU"
      },
      "outputs": [],
      "source": [
        "device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet,self).__init__()\n",
        "    self.conv1=nn.Conv2d(3,6,5)\n",
        "    self.conv2=nn.Conv2d(6,16,5)\n",
        "    self.fc1=nn.Linear(16*5*5,120)\n",
        "    self.fc2=nn.Linear(120,84)\n",
        "    self.fc3=nn.Linear(84,10)\n",
        "  def forward(self,x):\n",
        "    x=self.conv1(x)\n",
        "    x=F.relu(x)\n",
        "    x=F.max_pool2d(x,(2,2))\n",
        "    x=self.conv2(x)\n",
        "    x=F.relu(x)\n",
        "    x=F.max_pool2d(x,2)\n",
        "    x=x.view(-1,int(x.nelement()/x.shape[0]))\n",
        "    x=F.relu(self.fc1(x))\n",
        "    x=F.relu(self.fc2(x))\n",
        "    y=self.fc3(x)\n",
        "    return y\n",
        "model=LeNet().to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.manual_seed(42)\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_dataset = CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "train_dataset = CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "    # Testing loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f'Test Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(test_loader)}], Loss: {loss.item():.4f}')\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Tuto/model.pth')\n",
        "# Calculate the size of the model\n",
        "total_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "total_size_megabytes = total_size_bytes / (1024 * 1024)\n",
        "\n",
        "print(f\"Total size of the model: {total_size_megabytes:.2f} MB\")"
      ],
      "metadata": {
        "id": "F27z3TYwTbUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test before purned\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "           print(f'Model  Loss: {loss.item():.4f}')\n",
        "end_time = time.time()\n",
        "elapsed_time1 = end_time - start_time\n",
        "print(f\"Elapsed time: {elapsed_time1} seconds\")\n",
        "## Purned model\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "        #prune.l1_structured(module, name='weight', amount=0.3)\n",
        "        prune.random_unstructured(module,name=\"weight\",amount=0.1)\n",
        "# Calculate the size of the model\n",
        "total_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "total_size_megabytes = total_size_bytes / (1024 * 1024)\n",
        "#Test aftre purned\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "           print(f'Model purned Loss: {loss.item():.4f}')\n",
        "end_time = time.time()\n",
        "elapsed_time2 = end_time - start_time\n",
        "print(f\"Elapsed time: {elapsed_time2} seconds\")"
      ],
      "metadata": {
        "id": "QOOi_Pq6sqXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prune the model\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "        #prune.l1_structured(module, name='weight', amount=0.3)\n",
        "        prune.random_unstructured(module,name=\"weight\",amount=0.5)\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "       prune.remove(module, 'weight')\n",
        "# Calculate the size of the model\n",
        "total_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "\n",
        "total_size_megabytes = total_size_bytes / (1024 * 1024)\n",
        "print(f\"Total size of the model pruned: {total_size_megabytes:.2f} MB\")\n",
        "pruned_model_path = '/content/drive/MyDrive/Tuto/model_pruned.pth'\n",
        "torch.save(model.state_dict(), pruned_model_path)\n",
        "print(f\"Pruned model saved at: {pruned_model_path}\")\n",
        "pruned_model_path = 'model_pruned.pth'\n",
        "torch.save(model.state_dict(), pruned_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRzt1bRBgwgh",
        "outputId": "2e16f145-f132-4c2a-b1fc-7653ac814beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model pruned: 0.24 MB\n",
            "Pruned model saved at: /content/drive/MyDrive/Tuto/model_pruned.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "# Load the model\n",
        "model_path1= '/content/drive/MyDrive/Tuto/model.pth'\n",
        "state_dict1 = torch.load(model_path1)\n",
        "model_path2= '/content/drive/MyDrive/Tuto/model_pruned.pth'\n",
        "state_dict2 = torch.load(model_path2)\n",
        "# Remove the additional keys related to pruning\n",
        "state_dict2 = {key: value for key, value in state_dict2.items() if not key.endswith(('weight_orig', 'weight_mask'))}\n",
        "#Test model\n",
        "model1=LeNet()\n",
        "model1.load_state_dict(state_dict1)\n",
        "model1.eval()\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "        outputs = model1(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "           print(f'Model 1 Loss: {loss.item():.4f}')\n",
        "end_time = time.time()\n",
        "elapsed_time1 = end_time - start_time\n",
        "print(f\"Elapsed time: {elapsed_time1} seconds\")\n",
        "\n",
        "model2=LeNet()\n",
        "model2.load_state_dict(state_dict2)\n",
        "model2.eval()\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "        outputs = model2(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "           print(f'Model 2 Loss: {loss.item():.4f}')\n",
        "end_time = time.time()\n",
        "elapsed_time2 = end_time - start_time\n",
        "print(f\"Elapsed time: {elapsed_time2} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNsN1F_Fax1q",
        "outputId": "4247d4b1-4a2f-4d05-f9ca-b10f6f284418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 Loss: 0.6309\n",
            "Elapsed time: 5.255553483963013 seconds\n",
            "Model 2 Loss: 2.3238\n",
            "Elapsed time: 3.9305484294891357 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_size_bytes = sum(p.numel() * p.element_size() for p in model1.parameters())\n",
        "total_size_megabytes = total_size_bytes / (1024 * 1024)\n",
        "\n",
        "print(f\"Total size of the model1: {total_size_megabytes:.2f} MB\")\n",
        "total_size_bytes = sum(p.numel() * p.element_size() for p in model2.parameters())\n",
        "total_size_megabytes = total_size_bytes / (1024 * 1024)\n",
        "\n",
        "print(f\"Total size of the model2: {total_size_megabytes:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV2F4x4vgAve",
        "outputId": "c8654628-d9ba-4bd8-b754-f19cb46ad984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model1: 0.24 MB\n",
            "Total size of the model2: 0.24 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Quantization\n",
        "from torch.quantization import quantize_dynamic, QuantStub, DeQuantStub\n",
        "quantized_model = quantize_dynamic(model1, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "torch.save(quantized_model.state_dict(), 'quantized_model.pth')\n",
        "quantized_model.eval()\n",
        "model1.eval()\n",
        "l=0\n",
        "lq=0\n",
        "numtry=10\n",
        "for i in range(numtry):\n",
        "  with torch.no_grad():\n",
        "      for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "          outputsq = quantized_model(data)\n",
        "          outputs = model1(data)\n",
        "          lossq = criterion(outputsq, targets)\n",
        "          loss = criterion(outputs, targets)\n",
        "          if (batch_idx + 1) % 100 == 0:\n",
        "            #print(f'Model Loss: {loss.item():.4f} and Model quantized Loss: {lossq.item():.4f}')\n",
        "            lq+=lossq.item()\n",
        "            l+=loss.item()\n",
        "l=l/numtry\n",
        "lq=lq/numtry\n",
        "print(f'Model Loss: {l:.4f} and Model quantized Loss: {lq:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvYg7jYQC5fl",
        "outputId": "9a82d3dd-4ed6-451b-f293-5ac192f72350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Loss: 0.7011 and Model quantized Loss: 0.6988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "quantized_model = quantize_dynamic(model2, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "torch.save(quantized_model.state_dict(), 'quantized_purned_model.pth')\n",
        "torch.save(quantized_model.state_dict(), 'quantized_model.pth')\n",
        "torch.save(model1.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "uSm8Vi_VF7np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization Model\n"
      ],
      "metadata": {
        "id": "_ucE8xwoLyiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install onnx2pytorch\n",
        "!pip install onnx-torch\n",
        "!pip install torch-summary\n",
        "!pip install onnxruntime\n",
        "!pip install opencv-python\n",
        "!pip install pillow\n",
        "!pip install onnx\n",
        "!pip install onnxscript"
      ],
      "metadata": {
        "id": "Z9qpFZAVqPy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "model_fp32 = '/content/drive/MyDrive/Tuto/inceptdropfaith.onnx'\n",
        "model_quant = 'model.quant.onnx'\n",
        "quantized_model = quantize_dynamic(model_fp32, model_quant)\n"
      ],
      "metadata": {
        "id": "QuMHdvpsCb_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import onnx\n",
        "import onnxruntime\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "# Define the image preprocessing transforms\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(299),\n",
        "    transforms.CenterCrop(299),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define the list of class names\n",
        "class_names = ['D', 'P']\n",
        "\n",
        "# Load the ONNX model\n",
        "model_path1 = \"/content/drive/MyDrive/Tuto/inceptdropfaith.onnx\"\n",
        "model_path2 = \"/content/model.quant.onnx\"\n",
        "model1 = onnx.load(model_path1)\n",
        "model2 = onnx.load(model_path2)\n",
        "# Create a PyTorch model from the ONNX model\n",
        "pytorch_model1 = onnxruntime.InferenceSession(model_path1)\n",
        "pytorch_model2 = onnxruntime.InferenceSession(model_path2)\n",
        "\n",
        "input_name = pytorch_model.get_inputs()[0].name\n",
        "output_name = pytorch_model.get_outputs()[0].name\n",
        "\n",
        "# Load the image\n",
        "# Specify the path to the image\n",
        "image_path = '/content/drive/MyDrive/Tuto/Sans titre.jpeg'\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "# Convert the image to tensor\n",
        "#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "# Convert the OpenCV image to a PIL image\n",
        "img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "with torch.no_grad():\n",
        "    frame_tensor = preprocess(img).unsqueeze(0)  # Add channel dimension\n",
        "    frame_tensor = frame_tensor.cpu()  # Move to CPU if the model is on GPU\n",
        "\n",
        "    # Perform inference\n",
        "    output1 = pytorch_model1.run([], {input_name: frame_tensor.numpy()})[0]\n",
        "    output2 = pytorch_model2.run([], {input_name: frame_tensor.numpy()})[0]\n",
        "    # Get the predicted class and accuracy\n",
        "    pred_idx1 = output1.argmax()\n",
        "    pred_class1 = class_names[pred_idx1]\n",
        "    accuracy1 = max(output[0])\n",
        "    pred_idx2 = output2.argmax()\n",
        "    pred_class2 = class_names[pred_idx2]\n",
        "    accuracy2 = max(output[0])\n",
        "\n",
        "    # Print the results to console\n",
        "    print(f\"class={pred_class1}, accuracy={accuracy1:.2f}\")\n",
        "    print(f\"class={pred_class2}, accuracy={accuracy2:.2f}\")"
      ],
      "metadata": {
        "id": "koU7W3xqQE6C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}